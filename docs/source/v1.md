# TensorRT-LLM 1.0

```{note}
Note:
The 1.0 architecture is provided as a beta, and the related API is subject to change in future versions.
```

TensorRT-LLM v1.0 provides a new architecture using PyTorch and built with modular Python blocks to improve developer and deployer experience.

The new architecture is available to try as a beta starting in version 0.17. You can try it via importing `tensorrt_llm._torch`.

## Quick Start

Here is a simple example to show how to use `tensorrt_llm._torch.LLM` API with Llama model.

```{literalinclude} ../../examples/pytorch/quickstart.py
    :language: python
    :linenos:
```

## Quantization

The 1.0 architecture supports FP8 and NVFP4 quantization. You can pass quantized models in HF model hub,
which are generated by [TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer).

```python
from tensorrt_llm._torch import LLM
llm = LLM(model='nvidia/Llama-3.1-8B-Instruct-FP8')
llm.generate("Hello, my name is")
```

Or you can try the following commands to get a quantized model by yourself:

```bash
git clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git
cd TensorRT-Model-Optimizer/examples/llm_ptq
scripts/huggingface_example.sh --model <huggingface_model_card> --quant fp8 --export_fmt hf
```

## Developer Guide

- [Architecture Overview](./torch/arch_overview.md)
- [Adding a New Model](./torch/adding_new_model.md)

## Key Components

- [Attention](./torch/attention.md)
- [KV Cache Manager](./torch/kv_cache_manager.md)
- [Scheduler](./torch/scheduler.md)

## Known Issues

- The 1.0 architecture on SBSA is incompatible with bare metal environments like Ubuntu 24.04. Please use the [PyTorch NGC Container (https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) for optimal support on SBSA platforms.
